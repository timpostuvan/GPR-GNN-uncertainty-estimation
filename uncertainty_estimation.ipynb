{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "from torch_geometric.datasets import HeterophilousGraphDataset\n",
    "\n",
    "# Load dataset.\n",
    "dataset = HeterophilousGraphDataset(root=\"./data\", name=\"Roman-Empire\")\n",
    "data = dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GPR-GNN model.\n",
    "\n",
    "class GPRGNNConv(MessagePassing):\n",
    "    def __init__(self):\n",
    "        super().__init__(aggr='add')\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Add self-loops to the adjacency matrix.\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        # Compute normalization for symmetric normalized matrix.\n",
    "        row, col = edge_index\n",
    "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        # Propagate messages.\n",
    "        out = self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        # Normalize neighboring vectors.\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "\n",
    "class GPRGNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        hidden_channels: int,\n",
    "        K: int,\n",
    "        nn_dropout_p: float,\n",
    "        gpr_dropout_p: float\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Dropout(p=nn_dropout_p),\n",
    "            nn.Linear(in_channels, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=nn_dropout_p),\n",
    "            nn.Linear(hidden_channels, out_channels)\n",
    "        )\n",
    "\n",
    "        self.conv = nn.ModuleList([\n",
    "            GPRGNNConv() for _ in range(K)\n",
    "        ])\n",
    "\n",
    "        self.gpr_dropout = nn.Dropout(p=gpr_dropout_p)\n",
    "\n",
    "        self.gamma = nn.Parameter(torch.rand(size=(K + 1,)))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Project initial features with a MLP.\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        # Dropout to prevent overfitting.\n",
    "        x = self.gpr_dropout(x)\n",
    "\n",
    "        # Combine hidden representations with GPR weights.\n",
    "        z = self.gamma[0] * x\n",
    "        for i in range(K):\n",
    "            x = self.conv[i](x=x, edge_index=edge_index)\n",
    "            z = z + self.gamma[i + 1] * x\n",
    "\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Train: 0.2137, Val: 0.2104\n",
      "Epoch: 010, Train: 0.5041, Val: 0.4883\n",
      "Epoch: 020, Train: 0.5850, Val: 0.5626\n",
      "Epoch: 030, Train: 0.6495, Val: 0.6233\n",
      "Epoch: 040, Train: 0.6771, Val: 0.6501\n",
      "Epoch: 050, Train: 0.6942, Val: 0.6579\n",
      "Epoch: 060, Train: 0.7069, Val: 0.6669\n",
      "Epoch: 070, Train: 0.7137, Val: 0.6688\n",
      "Epoch: 080, Train: 0.7222, Val: 0.6757\n",
      "Epoch: 090, Train: 0.7312, Val: 0.6789\n",
      "FINAL PERFORMANCE OF EXPERIMENT 0: Train: 0.7347, Val: 0.6796, Test: 0.6784\n",
      "Epoch: 000, Train: 0.1679, Val: 0.1656\n",
      "Epoch: 010, Train: 0.4796, Val: 0.4722\n",
      "Epoch: 020, Train: 0.5631, Val: 0.5518\n",
      "Epoch: 030, Train: 0.6321, Val: 0.6199\n",
      "Epoch: 040, Train: 0.6679, Val: 0.6489\n",
      "Epoch: 050, Train: 0.6935, Val: 0.6609\n",
      "Epoch: 060, Train: 0.7091, Val: 0.6720\n",
      "Epoch: 070, Train: 0.7166, Val: 0.6763\n",
      "Epoch: 080, Train: 0.7220, Val: 0.6837\n",
      "Epoch: 090, Train: 0.7292, Val: 0.6824\n",
      "FINAL PERFORMANCE OF EXPERIMENT 1: Train: 0.7311, Val: 0.6810, Test: 0.6867\n",
      "Epoch: 000, Train: 0.1549, Val: 0.1511\n",
      "Epoch: 010, Train: 0.4525, Val: 0.4434\n",
      "Epoch: 020, Train: 0.5541, Val: 0.5372\n",
      "Epoch: 030, Train: 0.6186, Val: 0.5915\n",
      "Epoch: 040, Train: 0.6693, Val: 0.6374\n",
      "Epoch: 050, Train: 0.6947, Val: 0.6542\n",
      "Epoch: 060, Train: 0.7074, Val: 0.6623\n",
      "Epoch: 070, Train: 0.7171, Val: 0.6715\n",
      "Epoch: 080, Train: 0.7268, Val: 0.6736\n",
      "Epoch: 090, Train: 0.7267, Val: 0.6727\n",
      "FINAL PERFORMANCE OF EXPERIMENT 2: Train: 0.7308, Val: 0.6740, Test: 0.6770\n",
      "Epoch: 000, Train: 0.1850, Val: 0.1850\n",
      "Epoch: 010, Train: 0.4660, Val: 0.4577\n",
      "Epoch: 020, Train: 0.5859, Val: 0.5756\n",
      "Epoch: 030, Train: 0.6538, Val: 0.6258\n",
      "Epoch: 040, Train: 0.6927, Val: 0.6584\n",
      "Epoch: 050, Train: 0.7081, Val: 0.6680\n",
      "Epoch: 060, Train: 0.7148, Val: 0.6699\n",
      "Epoch: 070, Train: 0.7290, Val: 0.6754\n",
      "Epoch: 080, Train: 0.7359, Val: 0.6794\n",
      "Epoch: 090, Train: 0.7311, Val: 0.6754\n",
      "FINAL PERFORMANCE OF EXPERIMENT 3: Train: 0.7367, Val: 0.6747, Test: 0.6827\n",
      "Epoch: 000, Train: 0.2222, Val: 0.2164\n",
      "Epoch: 010, Train: 0.5255, Val: 0.5283\n",
      "Epoch: 020, Train: 0.6254, Val: 0.6056\n",
      "Epoch: 030, Train: 0.6735, Val: 0.6441\n",
      "Epoch: 040, Train: 0.6898, Val: 0.6551\n",
      "Epoch: 050, Train: 0.7033, Val: 0.6669\n",
      "Epoch: 060, Train: 0.7123, Val: 0.6701\n",
      "Epoch: 070, Train: 0.7218, Val: 0.6757\n",
      "Epoch: 080, Train: 0.7270, Val: 0.6800\n",
      "Epoch: 090, Train: 0.7331, Val: 0.6831\n",
      "FINAL PERFORMANCE OF EXPERIMENT 4: Train: 0.7388, Val: 0.6778, Test: 0.6770\n",
      "Epoch: 000, Train: 0.1452, Val: 0.1458\n",
      "Epoch: 010, Train: 0.4715, Val: 0.4687\n",
      "Epoch: 020, Train: 0.5795, Val: 0.5703\n",
      "Epoch: 030, Train: 0.6421, Val: 0.6307\n",
      "Epoch: 040, Train: 0.6746, Val: 0.6614\n",
      "Epoch: 050, Train: 0.6929, Val: 0.6667\n",
      "Epoch: 060, Train: 0.7015, Val: 0.6717\n",
      "Epoch: 070, Train: 0.7078, Val: 0.6710\n",
      "Epoch: 080, Train: 0.7215, Val: 0.6768\n",
      "Epoch: 090, Train: 0.7285, Val: 0.6775\n",
      "FINAL PERFORMANCE OF EXPERIMENT 5: Train: 0.7311, Val: 0.6778, Test: 0.6804\n",
      "Epoch: 000, Train: 0.2262, Val: 0.2245\n",
      "Epoch: 010, Train: 0.4590, Val: 0.4477\n",
      "Epoch: 020, Train: 0.5624, Val: 0.5412\n",
      "Epoch: 030, Train: 0.6328, Val: 0.6048\n",
      "Epoch: 040, Train: 0.6723, Val: 0.6387\n",
      "Epoch: 050, Train: 0.6880, Val: 0.6537\n",
      "Epoch: 060, Train: 0.7027, Val: 0.6618\n",
      "Epoch: 070, Train: 0.7088, Val: 0.6641\n",
      "Epoch: 080, Train: 0.7167, Val: 0.6695\n",
      "Epoch: 090, Train: 0.7280, Val: 0.6720\n",
      "FINAL PERFORMANCE OF EXPERIMENT 6: Train: 0.7310, Val: 0.6741, Test: 0.6730\n",
      "Epoch: 000, Train: 0.1395, Val: 0.1393\n",
      "Epoch: 010, Train: 0.5033, Val: 0.5024\n",
      "Epoch: 020, Train: 0.5801, Val: 0.5661\n",
      "Epoch: 030, Train: 0.6531, Val: 0.6265\n",
      "Epoch: 040, Train: 0.6898, Val: 0.6508\n",
      "Epoch: 050, Train: 0.7003, Val: 0.6598\n",
      "Epoch: 060, Train: 0.7139, Val: 0.6651\n",
      "Epoch: 070, Train: 0.7243, Val: 0.6711\n",
      "Epoch: 080, Train: 0.7299, Val: 0.6708\n",
      "Epoch: 090, Train: 0.7348, Val: 0.6748\n",
      "FINAL PERFORMANCE OF EXPERIMENT 7: Train: 0.7391, Val: 0.6741, Test: 0.6781\n",
      "Epoch: 000, Train: 0.1475, Val: 0.1462\n",
      "Epoch: 010, Train: 0.5341, Val: 0.5257\n",
      "Epoch: 020, Train: 0.6153, Val: 0.6028\n",
      "Epoch: 030, Train: 0.6608, Val: 0.6395\n",
      "Epoch: 040, Train: 0.6794, Val: 0.6531\n",
      "Epoch: 050, Train: 0.6958, Val: 0.6602\n",
      "Epoch: 060, Train: 0.7044, Val: 0.6655\n",
      "Epoch: 070, Train: 0.7125, Val: 0.6695\n",
      "Epoch: 080, Train: 0.7224, Val: 0.6706\n",
      "Epoch: 090, Train: 0.7240, Val: 0.6701\n",
      "FINAL PERFORMANCE OF EXPERIMENT 8: Train: 0.7352, Val: 0.6766, Test: 0.6777\n",
      "Epoch: 000, Train: 0.1634, Val: 0.1578\n",
      "Epoch: 010, Train: 0.4627, Val: 0.4461\n",
      "Epoch: 020, Train: 0.5467, Val: 0.5222\n",
      "Epoch: 030, Train: 0.6207, Val: 0.5894\n",
      "Epoch: 040, Train: 0.6629, Val: 0.6289\n",
      "Epoch: 050, Train: 0.6841, Val: 0.6431\n",
      "Epoch: 060, Train: 0.6934, Val: 0.6570\n",
      "Epoch: 070, Train: 0.7076, Val: 0.6616\n",
      "Epoch: 080, Train: 0.7161, Val: 0.6637\n",
      "Epoch: 090, Train: 0.7231, Val: 0.6621\n",
      "FINAL PERFORMANCE OF EXPERIMENT 9: Train: 0.7276, Val: 0.6627, Test: 0.6592\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters are set according to validation accuracy and my intuition, where starting\n",
    "# values were taken from papers: \"Adaptive Universal Generalized PageRank Graph Neural Network\"\n",
    "# and \"A critical look at the evaluation of GNNs under heterophily: are we really making progress?\".\n",
    "\n",
    "hidden_dim = 512        # hidden dimension for MLP\n",
    "K = 5                   # degree of polynomial in GPR\n",
    "nn_dropout_p = 0.5      # dropout for MLP\n",
    "gpr_dropout_p = 0.5     # dropout before calculation of GPR score\n",
    "\n",
    "num_experiments = 10\n",
    "num_epochs = 100\n",
    "learning_rate = 1e-2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def train(model, data, experiment_ind: int):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    train_mask = data.train_mask[:, experiment_ind]\n",
    "\n",
    "    logits = model(x=data.x, edge_index=data.edge_index)[train_mask]\n",
    "    loss = criterion(logits, data.y[train_mask])\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data, test_set: bool, experiment_ind: int):\n",
    "    model.eval()\n",
    "    logits, accs = model(x=data.x, edge_index=data.edge_index), []\n",
    "\n",
    "    data_splits = [\"train_mask\", \"val_mask\"]\n",
    "    if test_set:\n",
    "        data_splits.append(\"test_mask\")\n",
    "\n",
    "    for mask_name in data_splits:\n",
    "        mask = getattr(data, mask_name)\n",
    "        mask = mask[:, experiment_ind]\n",
    "\n",
    "        pred = logits[mask].max(dim=1)[1]\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        accs.append(acc)\n",
    "\n",
    "    return accs\n",
    "\n",
    "\n",
    "train_accs, val_accs, test_accs = [], [], []\n",
    "for experiment_ind in range(num_experiments):\n",
    "    # Reinitialize the model for each experiment.\n",
    "    model = GPRGNN(\n",
    "        in_channels=dataset.num_features,\n",
    "        out_channels=dataset.num_classes,\n",
    "        hidden_channels=hidden_dim,\n",
    "        K=K,\n",
    "        nn_dropout_p=nn_dropout_p,\n",
    "        gpr_dropout_p=gpr_dropout_p,\n",
    "    )\n",
    "\n",
    "    model, data = model.to(device), data.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model.\n",
    "    for epoch in range(0, num_epochs):\n",
    "        train(model, data, experiment_ind=experiment_ind)\n",
    "        if epoch % 10 == 0:\n",
    "            train_acc, val_acc = test(model, data, test_set=False, experiment_ind=experiment_ind)\n",
    "            print(f'Epoch: {epoch:03d}, Train: {train_acc:.4f}, Val: {val_acc:.4f}')\n",
    "    \n",
    "    # Evaluate the model on test set.\n",
    "    train_acc, val_acc, test_acc = test(model, data, test_set=True, experiment_ind=experiment_ind)\n",
    "    print(f'FINAL PERFORMANCE OF EXPERIMENT {experiment_ind}: Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "    test_accs.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OVERALL PERFORMANCE\n",
      "Train: 0.7336 ± 0.0037\n",
      "Val: 0.6753 ± 0.0048\n",
      "Test: 0.6770 ± 0.0069\n"
     ]
    }
   ],
   "source": [
    "# Calculate overall performance.\n",
    "train_accs = np.array(train_accs)\n",
    "val_accs = np.array(val_accs)\n",
    "test_accs = np.array(test_accs)\n",
    "\n",
    "print(f'OVERALL PERFORMANCE')\n",
    "print(f'Train: {np.mean(train_accs):.4f} ± {np.std(train_accs):.4f}')\n",
    "print(f'Val: {np.mean(val_accs):.4f} ± {np.std(val_accs):.4f}')\n",
    "print(f'Test: {np.mean(test_accs):.4f} ± {np.std(test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def uncertainty_estimation(model, data):\n",
    "    model.eval()\n",
    "    logits = model(x=data.x, edge_index=data.edge_index)\n",
    "    preds = nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Calculate entropies.\n",
    "    log_preds = torch.log2(preds)\n",
    "    log_preds[log_preds == float('-inf')] = 0\n",
    "    entropies = -torch.sum(preds * log_preds, dim=-1)\n",
    "\n",
    "    assert torch.all(entropies >= 0)    \n",
    "\n",
    "    test_entropies = entropies[data.test_mask]\n",
    "    ood_test_examples = data.ood_test_mask[data.test_mask].int()\n",
    "\n",
    "    auc = metrics.roc_auc_score(y_true=ood_test_examples, y_score=test_entropies)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-distribution  examples with features sampled from the standard normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with out-of-distribution examples.\n",
    "perturbed_data = copy.deepcopy(data)\n",
    "\n",
    "# Let's limit to only a single experiment since we want to perturb only features within test set.\n",
    "# Take the last experiment since we already have a trained model for that split.\n",
    "experiment_ind = 9\n",
    "perturbed_data.test_mask = perturbed_data.test_mask[:, experiment_ind]\n",
    "num_nodes, num_features = perturbed_data.x.shape\n",
    "\n",
    "# Choose approximately half of test nodes as out-of-distribution examples.\n",
    "ood_mask = torch.rand(size=(perturbed_data.x.shape[0],), device=perturbed_data.test_mask.device) < 0.5\n",
    "perturbed_data.ood_test_mask = perturbed_data.test_mask & ood_mask\n",
    "\n",
    "# Randomly sample features from the standard normal distribution for out-of-distribution examples.\n",
    "num_ood_test_examples, num_features = torch.sum(perturbed_data.ood_test_mask), perturbed_data.x.shape[1]\n",
    "perturbed_data.x[perturbed_data.ood_test_mask] = torch.normal(size=(num_ood_test_examples, num_features), mean=0, std=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNCERTAINTY AUC SCORE FOR EXPERIMENT 9: 0.10524300681624174\n"
     ]
    }
   ],
   "source": [
    "uncertainty_auc = uncertainty_estimation(model, perturbed_data)\n",
    "print(f\"UNCERTAINTY AUC SCORE FOR EXPERIMENT {experiment_ind}: {uncertainty_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-distribution examples with features sampled from normal distribution with the original mean and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with out-of-distribution examples.\n",
    "perturbed_data = copy.deepcopy(data)\n",
    "\n",
    "# Let's limit to only a single experiment since we want to perturb only features within test set.\n",
    "# Take the last experiment since we already have a trained model for that split.\n",
    "experiment_ind = 9\n",
    "perturbed_data.test_mask = perturbed_data.test_mask[:, experiment_ind]\n",
    "num_nodes, num_features = perturbed_data.x.shape\n",
    "\n",
    "# Choose approximately half of test nodes as out-of-distribution examples.\n",
    "ood_mask = torch.rand(size=(perturbed_data.x.shape[0],), device=perturbed_data.test_mask.device) < 0.5\n",
    "perturbed_data.ood_test_mask = perturbed_data.test_mask & ood_mask\n",
    "\n",
    "# Randomly sample features from the normal distribution with the original mean and std for out-of-distribution examples.\n",
    "num_ood_test_examples, num_features = torch.sum(perturbed_data.ood_test_mask), perturbed_data.x.shape[1]\n",
    "mean = torch.mean(perturbed_data.x, dim=0).reshape(1, -1)\n",
    "std = torch.std(perturbed_data.x, dim=0).reshape(1, -1)\n",
    "perturbed_data.x[perturbed_data.ood_test_mask] = torch.normal(mean=mean.repeat(num_ood_test_examples, 1), std=std.repeat(num_ood_test_examples, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNCERTAINTY AUC SCORE FOR EXPERIMENT 9: 0.8475519976914421\n"
     ]
    }
   ],
   "source": [
    "uncertainty_auc = uncertainty_estimation(model, perturbed_data)\n",
    "print(f\"UNCERTAINTY AUC SCORE FOR EXPERIMENT {experiment_ind}: {uncertainty_auc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
